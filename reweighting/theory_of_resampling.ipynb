{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ddfc7e7-89f4-4c3d-8de4-513c22c302e0",
   "metadata": {},
   "source": [
    "# PolicyEngine's Survey Reweighting - The Theory and Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216cb6f2-bdbf-4dba-b9e8-399fe13db9ef",
   "metadata": {},
   "source": [
    "## Totals of Finite Populations\n",
    "\n",
    "### What really is a total?\n",
    "In introductory Statistics, we are introduced to models of data such as $X_i \\sim \\text{N}(\\mu, \\sigma^2)$ or $Y_i \\sim \\text{Bernoulli}(p)$, for an i.i.d. sample $i=1, \\dots, n$. The sample total, $\\sum_{i=1}^n X_i$ or $\\sum_{i=1}^n Y_i$, is noteable in its sufficiency for estimation of $\\mu$ or $p$, but aside from using it to create an unbiased estimator of $\\mu$ or $p$, it's hard to imagine a situation where the sample statistic by itself is especially meaningful. For example, if $Y_i$ is the income of household $i$ and $n$ is 100, then the total is the sum of the incomes of 100 households. It's clear that it's meaningless.\n",
    "\n",
    "On the other hand, imagine that the households are the complete list of households in the United States. Then the total is the aggregate income of all households in the US in a given year, an important quantity to economists. What is fundamentally different about this example?\n",
    "\n",
    "### Finite populations\n",
    "The difference is that in the second example, the population is *finite*. That might not sound like a big deal, or it might even sound like a limitation, but think about why the total lacks intuitive meaning in the first example. By way of these continuous and discrete distributions being *infinite population* models, the individuals drawn are arbitrary. You could draw 100 or 1000 or a trillion other individuals just like them, and in that sense there is no real concept of a total in the infinite population problem formulation.\n",
    "\n",
    "When there is a finite population, there is a clear concept of a total. When we estimate the total of a finite population, the parameter is a function of the potentially observed data, $T = y_1 + y_2 + \\ldots + y_N$, which is also strange when one is used to estimating theoretical parameters than describe the generation of the data. A final unique feature of the finite population approach is that, if all $y_i$ are observed, $T$ is estimated perfectly, with no uncertainty at all. This is the basis for the finite population correction factor when computing the standard error of estimates for totals.\n",
    "\n",
    "### Sampling from finite populations\n",
    "In infinite-population models like the normal and bernoulli models introduced at the beginning, values are drawn from distributions, but there is no concept of a probability of selection of an individual, and, if we insisted upon one, it would be zero. In the finite population sampling case, probabilities of selection are non-zero and determined by the mechanistic design of the sampling process.\n",
    "\n",
    "Perhaps the best known mechanism is randomly sampling without replacement from a population, which in the real world could be approximately implemented with shuffled paper entries in a hat. (Here \"random\" refers to equally likely and not of a random variable.) This is *Simple Random Sampling without Replacement*, often just called *Simple Random Sampling* (SRS). When the population size is $N$ and the sample size is $n$, it can be easily proven that each unit has a probability of selection of $\\pi_i = n / N$.\n",
    "\n",
    "If an SRS is taken separately for two or more groups, also called \"strata,\" this becomes *Stratified Random Sampling*. Assuming different stratum sample sizes, the probabilities of selection are no longer equal. Considering two strata of size $N_1$ and $N_2$ such that population size $N=N_1 + N_2$, and sample sizes of $n_1$ and $n_2$ respectively, the probabilities of selection are $n_1 / N_1$ and $n_2 / N_2$.\n",
    "\n",
    "A general purpose estimator for finite population totals with known probabilities of selection is the Horvitz-Thompson estimator, or $$\\hat{T} = \\sum_{i=1}^n y_i / \\pi_i,$$ where $\\pi_i$ is the probability that unit $i$ is in the sample according to the design. If we define a weight as $w_i = 1 / \\pi_i$, then the Horvitz-Thompson estimator becomes $$\\hat{T} = \\sum_{i=1}^n w_i y_i,$$ and, given probabilities are between 0 and 1, arrives at the interpretation of weights $w_i$ as the number of units in the population each unit in the sample represents, which will be at minimum 1, and, ignoring a zero selection probability for the moment, has no upper bound.\n",
    "\n",
    "### Finite population sampling allows for nonparametric design-based inference\n",
    "\n",
    "Because, unlike the infinite population situation, there is a sampling method that enables units to be selected with known probabilities, a method of inference is available that is virtually assumption-free. This is called \"design-based inference.\" The reason that this is possible is that the random variables that are modeled are not the data, $y_i$, but rather the binary selection indicators $Z_i$ that determine whether unit $i$ was selected by the sampling mechanism. Yes, $Z_i$ is itself a bernouli random variable with $\\Pr(Z_i = 1) = \\pi_i$. Treating $y_i$ as fixed, unknown constants, we can see that the expectation of the Horvitz-Thompson estimator is $$ \\text{E}(\\sum_{i=1}^n y_i / \\pi_i) = \\sum_{i=1}^N \\text{E}(Z_i y_i / \\pi_i),$$ where, notice how the inclusion of the $Z_i$ allows the entire population to enter in the sum. The linearity of the expectation allows it to distribute through the sum, and since $y_i$ and $\\pi_i$ are constants, we have $$\\sum_{i=1}^N \\text{E}(Z_i y_i / \\pi_i) = \\sum_{i=1}^N \\text{E}(Z_i) y_i / \\pi_i = \\sum_{i=1}^N \\pi_i y_i / pi_i$$, where the last inequality follows because $\\text{E}(Z_i) = \\Pr(Z_i = 1)$. Thus, the $\\pi_i$ terms cancel out and we can see that the expectation of the Horvitz-Thompson estimator is the population total $T$ (over all $i = 1, \\ldots, N$).\n",
    "\n",
    "### Pros and cons of the design-based approach\n",
    "\n",
    "#### Pros: Automated analysis that you can trust\n",
    "Just because design-based finite population sampling theory gives us a way out of modeling our metric data, $y_i$, doesn't mean we have to use it. As we'll see, we still can, and perhaps still should, model our data.\n",
    "\n",
    "But if you're building an automated system of analysis and you have access to inclusion probabilities (or equivalently, sample weights), you have the ability to build a hands-free (really head-free) analysis platform. Since the probabilities are at the unit level, the total for any variable measured on the unit, $y_i$, or $x_i$, or $z_i$, whether one is normally distributed and one is binomial and one is cauchy, the exact same formulas and procedures apply. This may sound like what you get with the central limit theorem in infinite-population problem formulations, but this is true at every sample size, and for any method that is derived within the finite-population framework. Furthermore, the cauchy distributed variable was added because it is a pathological case where the central limit theorem breaks down. But the values produced by the draws from the cauchy distribution are no problem for design-based inference, in theory at least, for they are merely constants that make up part of a total.\n",
    "\n",
    "#### Cons: Higher variability esimation, sometimes wildly higher\n",
    "In his 1971 article \"An Essay on the Logical Foundations of Survey Sampling\" (TODO - check), Indian statistician Debabrata Basu introduced a scenario where, despite being unbiased, the Horvitz-Thompson estimator yields absurd results due to high variance.\n",
    "\n",
    "In the example, a circus owner wants to estimate the total weight of 50 elephants. For simplicity, he considers weighing just one elephant—Sambo—known to be roughly average, and multiplying that weight by 50. A statistician, aiming for a more \"rigorous\" design-based method, proposes selecting Sambo with probability 99/100, and distributing the remaining 1/100 equally across the other 49 elephants (each with inclusion probability 1/4900). If Sambo is selected and weighs 4,000 pounds, the Horvitz-Thompson estimate is $4,000 \\cdot 1\\ 0.99 \\approx 4,040$\n",
    "pounds for the total. If instead the largest elephant, Jumbo, is selected and weighs 5,000 pounds, the estimate becomes $ 5,000 \\cdot 1 / (1/4900) = 24,500,000$ pounds. Although these two possible outcomes are wildly different, the expected value of the estimator across all possible samples still equals the true total weight.\n",
    "\n",
    "The method, as rediculous as it is, can still be unbiased because a rare, very large estimate is averaged with frequent very small estimates, and the average across all possible samples is the true average. But that comes as little consolation when there is only one sample selected. The variance of the estimator matters.\n",
    "\n",
    "The design is also intentionally poor in a way that no serious survey would ever be. By overconcentrating inclusion probability on a single unit (Sambo), the design ensures high between-sample variability: in the rare event that Sambo is not selected, the Horvitz-Thompson estimate becomes extreme. This happens not only when Jumbo is selected—causing the estimate to explode—but also when any of the other 48 elephants is selected, due to their tiny inclusion probabilities. As Basu famously remarked, \"A bad design cannot be rescued by a good estimator, but a good estimator can be ruined by a bad design.\"\n",
    "\n",
    "### Models of the data\n",
    "\n",
    "Up until this section, the models of the data, to the extent there were any at all, were models of the sampling process via the random selection indicator $Z_i$. But what if we insisted on modeling the $y_i$s? Our job is still the same, to estimate the population total with only a sample. But, with a model of $y_i$ we feel comfortable with, say $$y_i = \\tau_1 \\text{I}(i \\in \\text{Stratum 1}) + \\tau_2 \\text{I}(i \\in \\text{Stratum 2}) + \\epsilon_i,$$ where $\\epsilon_i \\sim N(0, \\sigma^2)$, we can pursue a different strategy. Our model assumes that, within a stratum, the unit that's selected is indeed arbitrary. Note the use of the infinite-population model, called a \"superpopulation model,\" for $\\epsilon_i$. So if we believe this model, every unit sampled in Stratum 1 and Stratum 2 as good as a random draw from an infinite population. And thus, we can estimate model parameters $\\tau_1$, $\\tau_2$, and $\\sigma^2$ from our sample using ordinary regression techniques.\n",
    "\n",
    "So now you have a fitted model from a sample, how does that get you to estimating a total? Well, you know how many units you didn't sample ($N_1 - n_1$ and $N_2 - n_2$), and you have a model for those units, so you can use the model to predict the missing values. This is essentially missing data imputation. So the estimate of the total would become $$\\text{E}(\\sum_{i=1}^N y_i) = \\sum_{i \\in S} y_i + \\sum_{i \\notin S} \\text{E}(y_i),$$ where $\\text{E}(y_i)$ is the expectation *with respect to the model* (and not the sampling mechanism).\n",
    "\n",
    "In the case of stratified random sampling and this particular model, the model-based and design-based estimators coincide in form. However, the estimated variance differs, and more importantly, the interpretation of the inference differs: the design-based approach conditions on the data and averages over the sampling design, while the model-based approach conditions on the design and assumes the model generates the data. If the model were specified differently—say, using covariates rather than strata—then the model-based estimator would generally differ in form from the design-based estimator.\n",
    "\n",
    "### Weights in model-based inference\n",
    "\n",
    "In model-based analysis, we typically assume that, given the covariates, each observation is equally informative and errors have constant variance. Under these conditions, unweighted estimators are unbiased and, by the Gauss–Markov theorem, have the smallest variance among all linear unbiased estimators. If the model explicitly includes heteroskedastic errors with known or accurately modeled variances, weighting each observation by the inverse of its variance becomes optimal, as this restores efficiency and provides the lowest possible variance for a linear unbiased estimator.\n",
    "\n",
    "In contrast, using weights when each observation is equally informative and errors have constant variance will inflate variance of the estimators considered. This is captured by the \"design-effect\" formula $$1 + \\text{CV}(w)^2,$$ where $\\text{CV}$ is the coeffient of variation. So, absent a clear reason to use unequal weights, when dealing with model-based estimators, it is generally best to use equal weights (i.e., to not use them at all). \n",
    "\n",
    "It's not that weights and models never work together. \"Doubly Robust\" models use design-based weights together with a model of the data to get \"two chances to get it right.\" Population model parameters can be estimated in a design-based way, though the estimate represents the value of the parameter estimate if the same estimation procedure was applied to the whole population. (This is somewhat controversial, since, if a model's parameters are worth estimating in the population, why not use it as a superpopulation model?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a2f6d-1cdd-4dc5-b0c0-7f4fad152603",
   "metadata": {},
   "source": [
    "## Stratified Sampling Example: Design-Based vs. Model-Based Estimation\n",
    "\n",
    "In the code blocks that follow, notice how the design-based and model-based estimators both lead to the same estimate, even when the latter approach did not use weights. The variances are different under the different models, but the current focus is point estimation.\n",
    "\n",
    "\\begin{array}{rcccc}\n",
    "\\textbf{unit} & \\textbf{stratum} & \\textbf{y} & \\textbf{pr\\_select} & \\textbf{w} \\\\\n",
    "\\hline\n",
    "1 & A & 4 & 0.666667 & 1.5 \\\\\n",
    "2 & A & 3 & 0.666667 & 1.5 \\\\\n",
    "3 & A & 5 & 0.666667 & 1.5 \\\\\n",
    "4 & B & 8 & 0.500000 & 2.0 \\\\\n",
    "5 & B & 9 & 0.500000 & 2.0 \\\\\n",
    "6 & B & 7 & 0.500000 & 2.0 \\\\\n",
    "7 & B & 11 & 0.500000 & 2.0 \\\\\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d024f33-8d19-4eb0-bd50-dc0ed7491eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   unit stratum   y  pr_select    w\n",
      "0     1       A   4   0.666667  1.5\n",
      "1     2       A   3   0.666667  1.5\n",
      "2     3       A   5   0.666667  1.5\n",
      "3     4       B   8   0.500000  2.0\n",
      "4     5       B   9   0.500000  2.0\n",
      "5     6       B   7   0.500000  2.0\n",
      "6     7       B  11   0.500000  2.0\n",
      "The Population total of y is 47\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "population_df = pd.DataFrame(\n",
    "    {\n",
    "        \"unit\": [1, 2, 3, 4, 5, 6, 7],\n",
    "        \"stratum\": [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"],\n",
    "        \"y\": [4, 3, 5, 8, 9, 7, 11]\n",
    "    }\n",
    ")\n",
    "\n",
    "N_a = 3\n",
    "N_b = 4\n",
    "\n",
    "n_a = 2\n",
    "n_b = 2\n",
    "\n",
    "# We have to sample to make it true\n",
    "population_df[\"pr_select\"] = np.concatenate([\n",
    "    np.repeat(n_a / N_a, N_a),\n",
    "    np.repeat(n_b / N_b, N_b)\n",
    "])\n",
    "population_df[\"w\"] = 1 / population_df[\"pr_select\"]\n",
    "\n",
    "print(population_df)\n",
    "print(f\"The Population total of y is {np.sum(population_df.y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b608ece1-a360-4dc7-bf06-95b2a3e2bd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   unit stratum   y  pr_select    w\n",
      "0     1       A   4   0.666667  1.5\n",
      "1     2       A   3   0.666667  1.5\n",
      "4     5       B   9   0.500000  2.0\n",
      "6     7       B  11   0.500000  2.0\n",
      "Horvitz-Thompson total estimate: 50.5\n"
     ]
    }
   ],
   "source": [
    "# keeping it simple with n = n_a = n_b = 2\n",
    "sample = (\n",
    "    population_df\n",
    "        .groupby('stratum')\n",
    "        .sample(n=2, random_state=42)\n",
    ")\n",
    "\n",
    "print(sample)\n",
    "\n",
    "ht_total = (sample['y'] * sample['w']).sum()\n",
    "print(\"Horvitz-Thompson total estimate:\", ht_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "415a8dd0-d0c3-4927-b6c4-e7618ac21722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model-based total estimate: 50.5\n"
     ]
    }
   ],
   "source": [
    "# OLS for the stratified model is just the stratum means\n",
    "stratum_means = (\n",
    "    sample.groupby(\"stratum\")[\"y\"]\n",
    "    .mean()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "sample_sum = np.sum(sample.y)\n",
    "N_a_not_sampled = N_a - n_a\n",
    "\n",
    "not_sampled_sum_hat = (\n",
    "    N_a_not_sampled * stratum_means[\"A\"]\n",
    "    + N_b_not_sampled * stratum_means[\"B\"]\n",
    ")\n",
    "model_based_total = sample_sum + not_sampled_sum_hat\n",
    "\n",
    "print(f\"Model-based total estimate: {model_based_total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d606180-c6da-490d-b451-5f70fbee0521",
   "metadata": {},
   "source": [
    "## Poststratification and Local Areas\n",
    "\n",
    "In our stratified random sampling example above, the two strata were known ahead of time and fully incorporated into the design. While this is ideal, it does not preclude the researcher from estimating a local area total that was not incorporated into the design but identified later as a subpopulation of interest. In our simple population of 7 units, let's add a local area that's fully nested within stratum B:\n",
    "\n",
    "\\begin{array}{rcccc}\n",
    "\\textbf{unit} & \\textbf{stratum} & \\textbf{local area} & \\textbf{y} & \\textbf{pr\\_select} & \\textbf{w} \\\\\n",
    "\\hline\n",
    "1 & A & 0 & 4 & 0.666667 & 1.5 \\\\\n",
    "2 & A & 0 & 3 & 0.666667 & 1.5 \\\\\n",
    "3 & A & 0 & 5 & 0.666667 & 1.5 \\\\\n",
    "4 & B & 1 & 8 & 0.500000 & 2.0 \\\\\n",
    "5 & B & 1 & 9 & 0.500000 & 2.0 \\\\\n",
    "6 & B & 0 & 7 & 0.500000 & 2.0 \\\\\n",
    "7 & B & 0 & 11 & 0.500000 & 2.0 \\\\\n",
    "\\end{array}\n",
    "\n",
    "From this local area, we have the weight and value from Unit 5 in the sample. To compute the estimate of a total for a local area, we just take the weighted sum of the sampled units within that local area. In this very simple case, that weighted sum is the single term $$\\begin{align}\\hat{T}_{l} &= w_5 y_5 \\\\ &= 2 \\cdot 9 \\\\ &= 18, \\end{align}$$ not so far from the actual local area total of $8 + 9 = 17$.\n",
    "\n",
    "While the mechanics of estimation after poststratification are very simple, it is ideal to incorporate all analysis strata at design time. Perhaps the most obvious reason is that it is possible that no units, or an insufficient number of units, will be sampled from the local areas of interest when they are not an explicit part of the design. **TODO: n is random now - does that incorporate itself into the Variance estimates?** Also, creating strata opportunitistically (to achieve a certain outcome) introduces the same risks that come along with p-hacking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca1b8a-ec72-4c0a-853d-ae2ddb3883c0",
   "metadata": {},
   "source": [
    "## Calibration - Matching known totals\n",
    "\n",
    "### An introduction to calibration through the technique of raking\n",
    "\n",
    "#### 1-D raking\n",
    "Continuing with our simple population of 7 units, suppose that, from official sources or otherwise, the strata totals became known. That is, the analyst is made aware that $T_A = 12$ and $T_B = 35$. In such a small population, this constitutes a substantial reduction in uncertainty regarding the unknown values. For one, the only unsampled value of strata A can be mathematically derived. Also, the population total $T=47$ is now known, so that estimation of the grand total is no longer necessary. But there is still uncertainty in the local area total (there is exactly one degree of freedom left over after fixing the 2 sampled values and the total of stratum B). How could we incorporate the additional knowlege to improve our local area total estimate?\n",
    "\n",
    "One very intuitive idea is to proportionally adjust the weights so that total estimates match their known values. For stratum B, all weights are 2, and the stratum B total estimate of $\\hat{T}_B = 2 \\cdot 9 + 2 \\cdot 11 = 40,$ somewhat larger than the known value of 35. But if we multiply all weights in the stratum by $35 / 40 = 0.875$, the new calibrated weights for the stratum are $2.0 \\cdot 0.875 = 1.75$. This allows us to estimate our local area total again with calibrated weights, and the esimate is $$\\begin{align}\\hat{T}_{l} &= w^{\\star}_5 y_5 \\\\ &= 1.75 \\cdot 9 \\\\ &= 15.75.\\end{align}$$\n",
    "\n",
    "In this case, the calibrated estimate undershoots the true local area total by 1.25 whereas the original overshot the true total by only 1. This was an unintentional outcome in the construction of this example, but it points out that calibrated estimates are not guaranteed to be closer to their true values, even though that they are generally expected to be. In this case, Unit 7 constituted one half of stratum B's sample and it was the largest value in the stratum. Thus, the weights had to be adjusted down to meet the target. On average, in a repeated sampling experiment, the calibrated estimate of the local area total would be closer to the true total.\n",
    "\n",
    "#### Beyond 1-D raking\n",
    "Raking can be done beyond the one dimension of stratum totals that we had in this example, and include multiple categorical dimensions. When this is the case, the weights are said to be adjusted \"marginally\" (that is, focusing on one categorical dimension at a time (**TODO - check this statement**) and the adjustment proceeds sequentially until all totals are matched within a sufficient tolerance. If this tolerance is not reached, then the procedure is said to fail to converge. In this way, raking uses an \"infinite loss,\" where the totals are either matched perfectly or not at all.\n",
    "\n",
    "Researchers have relaxed the infinite loss requirement of raking and have come up with analogs that rely on other loss functions. An example is [Regularized Raking](TODO) which uses. A common thread between these looser versions of raking and the original is that, to the extent possible, weights are modified in a way that preserves their properties (**TODO: get help with this**). In the next section, we will remove even that restriction.\n",
    "\n",
    "\n",
    "### Reweighting: solving for weights using generalized regression techniques\n",
    "First, it is important to distinguish between *TODO: weighting regression *, a similar technique to raking that uses regression, and the meaning of regression in this section, which is to solve for weights as if they were linear regression parameters. It is different in spirit from the raking-adjacent methods because we do not seek to merely adjust the weights, we seek to find them using direct optimization, potentially using the original weights as starting values when they are needed. This is first section so far where we are at the frontier of known practice, so there is less published theory to fall back on.\n",
    "\n",
    "What if, in the case of our simple 7 unit population, we wished to encode all of our constraints in the regression notation of $\\mathbf{y} = \\mathbf{X \\beta}$. Though there are clearly dependencies in what follows, consider the following representation of the constraints put on the weights by the known totals. Recall that our sample consisted of units 1, 2, 5, 7 and note that the following will be set to be true.\n",
    "\n",
    "$$\\left(\\begin{matrix}\n",
    "T \\\\\n",
    "T_A \\\\\n",
    "T_B \\\\\n",
    "\\end{matrix} \\right) =\n",
    "\\left(\\begin{matrix}\n",
    "y_1 & y_2 & y_5 & y_7 \\\\\n",
    "y_1 & y_2 & 0 & 0 \\\\\n",
    "0 & 0 & y_5 & y_7\n",
    "\\end{matrix} \\right)\n",
    "\\left(\\begin{matrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_5 \\\\\n",
    "w_7\n",
    "\\end{matrix} \\right)\n",
    "$$\n",
    "Though at this time, $T = T_A + T_B,$ this will soon be relaxed. Additionally, it is helpful to see the matrix representation of both population total and stratum total estimates, and the rank-reducing linear dependency this entails. From this point on, rather than working with $\\mathbf{y} = \\mathbf{X \\beta}$, we will be working with $\\mathbf{t} = \\mathbf{M \\omega}$ to emphasize the subject matter.\n",
    "\n",
    "We cannot solve for $\\omega$ using the analytical least squares formula $(\\mathbf{M}^T \\mathbf{M})^{-1} \\mathbf{M}^T t$ because $(\\mathbf{M}^T \\mathbf{M})^{-1}$ is not invertable. However, we can use a Moore-Penrose generalized inverse to solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9ad3e74-3404-4a48-9889-55607d7a8a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The metric Matrix M is\n",
      "[[ 4  3  9 11]\n",
      " [ 4  3  0  0]\n",
      " [ 0  0  9 11]]\n",
      "\n",
      "The Moore-Penrose Generalized Inverse of M is\n",
      "[[ 0.05333333  0.10666667 -0.05333333]\n",
      " [ 0.04        0.08       -0.04      ]\n",
      " [ 0.01485149 -0.01485149  0.02970297]\n",
      " [ 0.01815182 -0.01815182  0.03630363]]\n",
      "\n",
      "A solution for the weights based on the generalized inverse is\n",
      "[1.92       1.44       1.55940594 1.90594059]\n",
      "\n",
      "We can see that our solution satisfies the known targets\n",
      "Total Estimate: 47.000000000000014\n",
      "Stratum A Total Estimate: 12.000000000000004\n",
      "Stratum B Total Estimate: 35.000000000000014\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "t = np.array([47, 12, 35])\n",
    "M = np.array([[4, 3, 9, 11], [4, 3, 0, 0], [0, 0, 9, 11]])\n",
    "\n",
    "print(\"The metric Matrix M is\")\n",
    "print(M)\n",
    "\n",
    "M_ginv = np.linalg.pinv(M)\n",
    "print(\"\\nThe Moore-Penrose Generalized Inverse of M is\")\n",
    "print(M_ginv)\n",
    "\n",
    "w_mp = M_ginv @ t\n",
    "print(\"\\nA solution for the weights based on the generalized inverse is\")\n",
    "print(w_mp)\n",
    "\n",
    "print(\"\\nWe can see that our solution satisfies the known targets\")\n",
    "print(f\"Total Estimate: {w_mp[0] * 4 + w_mp[1] * 3 + w_mp[2] * 9 + w_mp[3] * 11}\")\n",
    "print(f\"Stratum A Total Estimate: {w_mp[0] * 4 + w_mp[1] * 3}\")\n",
    "print(f\"Stratum B Total Estimate: {w_mp[2] * 9 + w_mp[3] * 11}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c01da9-a285-42aa-8087-4bd4535268a9",
   "metadata": {},
   "source": [
    "#### Infinite Solutions and the nullspace\n",
    "Aside from some rounding error, the weights found above lead to total estimates that exactly match the known totals, just like they did with 1-D raking. That is, there are at least two solutions that match the totals perfectly. Dropping the linear dependency, our 2 equations and 4 unknowns leads to an infinite number of weighting solutions. We can explore these solutions by navigating through the null-space (of what? **TODO: I'm rusty here. Relearn and rewrite**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3735b2f-9053-43a5-acfc-afdb728c9f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let's look at potential w_alt:\n",
      "[2.08300982 1.22265357 2.05771804 1.4982307 ]\n",
      "\n",
      "We can see that w_alt solution satisfies the known targets\n",
      "Total Estimate: 47.000000000000014\n",
      "Stratum A Total Estimate: 12.000000000000004\n",
      "Stratum B Total Estimate: 35.00000000000001\n",
      "\n",
      "Let's look at potential w_alt:\n",
      "[ 0.28990179  3.61346428 -3.42371503  5.98303957]\n",
      "\n",
      "We can see that w_alt solution satisfies the known targets\n",
      "Total Estimate: 47.00000000000004\n",
      "Stratum A Total Estimate: 12.000000000000007\n",
      "Stratum B Total Estimate: 35.00000000000003\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "z = rng.normal(size=4)  # A random direction in 4-D (there are 4 sample weights)\n",
    "\n",
    "coef = np.linalg.solve(M @ M.T, M @ z)\n",
    "z_star = z - M.T @ coef  # project z into null space (M z = 0)\n",
    "\n",
    "rho = 1.5\n",
    "w_alt_1 = w_mp + rho * z_star\n",
    "rho = -15\n",
    "w_alt_2 = w_mp + rho * z_star\n",
    "\n",
    "for w_alt in [w_alt_1, w_alt_2]:\n",
    "    print(\"\\nLet's look at potential w_alt:\")\n",
    "    print(w_alt)\n",
    "    print(\"\\nWe can see that w_alt solution satisfies the known targets\")\n",
    "    print(f\"Total Estimate: {w_alt[0] * 4 + w_alt[1] * 3 + w_alt[2] * 9 + w_alt[3] * 11}\")\n",
    "    print(f\"Stratum A Total Estimate: {w_alt[0] * 4 + w_alt[1] * 3}\")\n",
    "    print(f\"Stratum B Total Estimate: {w_alt[2] * 9 + w_alt[3] * 11}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb018a-65c4-479d-b35f-cb8360ae72da",
   "metadata": {},
   "source": [
    "By projecting a random direction into the nullspace of M, we now have a parameterization of possible solutions facilitated by modulating `rho`. When we made `rho` equal to -15, however, we saw that negative weights are possible. While the use of negative weights in finite population sampling is not unheard of, it is unintuitive. Especially when solutions exist in the space of all positive weights (and even when they don't, as we shall see), it is beneficial to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe96b51-3851-4e7a-8cb5-d895a76c13f0",
   "metadata": {},
   "source": [
    "## Nonlinear solvers for positive weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a02befb6-4e0f-489f-a8c5-a00a79d8a728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss at epoch 0 is 39.5\n",
      "The loss at epoch 10 is 4.767368793487549\n",
      "The loss at epoch 20 is 2.2563726902008057\n",
      "The loss at epoch 30 is 0.29043981432914734\n",
      "The loss at epoch 40 is 0.008989228866994381\n",
      "The loss at epoch 50 is 0.006177504546940327\n",
      "The loss at epoch 60 is 0.004472102038562298\n",
      "The loss at epoch 70 is 0.004076426383107901\n",
      "The loss at epoch 80 is 0.0036225493531674147\n",
      "The loss at epoch 90 is 0.001115654595196247\n",
      "\n",
      "Let's examine the weights vector after 100 epochs\n",
      "tensor([1.7139, 1.7139, 1.7501, 1.7501], requires_grad=True)\n",
      "\n",
      "We can see that the pytorch solution satisfies the known targets\n",
      "Total Estimate: 46.99851989746094\n",
      "Stratum A Total Estimate: 11.997053146362305\n",
      "Stratum B Total Estimate: 35.001468658447266\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w_tensor = torch.tensor([1.5, 1.5, 2.0, 2.0], requires_grad=True)\n",
    "t_tensor = torch.tensor([47.0, 12, 35])\n",
    "M_tensor = torch.tensor([[4.0, 3, 9, 11], [4, 3, 0, 0], [0, 0, 9, 11]])\n",
    "\n",
    "def mse_loss(w):\n",
    "    return torch.sum((torch.matmul(M_tensor, w_tensor) - t_tensor) ** 2)\n",
    "\n",
    "optimizer = torch.optim.Adam([w_tensor], lr=0.15)\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss_value = mse_loss(w_tensor)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"The loss at epoch {epoch} is {loss_value}\")\n",
    "\n",
    "print(f\"\\nLet's examine the weights vector after {n_epochs} epochs\")\n",
    "print(w_tensor)\n",
    "w_alt = w_tensor.detach().numpy()\n",
    "print(\"\\nUp to rounding, the pytorch solution satisfies the known targets\")\n",
    "print(f\"Total Estimate: {w_alt[0] * 4 + w_alt[1] * 3 + w_alt[2] * 9 + w_alt[3] * 11}\")\n",
    "print(f\"Stratum A Total Estimate: {w_alt[0] * 4 + w_alt[1] * 3}\")\n",
    "print(f\"Stratum B Total Estimate: {w_alt[2] * 9 + w_alt[3] * 11}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ffe37-d797-47b6-98a8-8a140507c9f3",
   "metadata": {},
   "source": [
    "Pytorch is a very powerful nonlinear optimizer that can be extended to any problem in this problem space. Note that, having started from the original weights, the final weights are almost exactly those that resulted from the raking calibration algorithm. Whether this is a coincidence is unclear at this time (*could we prove that the raking solution is a local optimum when starting at the original weights?*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca34e1-0bf8-416d-81ef-c9822232d4bf",
   "metadata": {},
   "source": [
    "### Contradictions: when perfection isn't possible\n",
    "\n",
    "To this point, we've taken for granted that the \"known\" population and strata totals were in fact known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed31cf-b926-45b9-9a58-5681b9dc43ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code example of Pytorch with imperfect information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eae527-ea1c-4872-8fa3-91ee6cf0a77a",
   "metadata": {},
   "source": [
    "### A framework for contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54c4913e-1ccd-446c-9266-c5128ea906fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 4, 1, 3, 5, 4],\n",
       "       [5, 2, 2, 1, 3, 7]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.random.multinomial(20, [1/6.]*6, size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f9da8-53d6-4f8b-b30d-85490394742e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
