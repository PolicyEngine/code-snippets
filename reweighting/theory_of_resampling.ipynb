{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ddfc7e7-89f4-4c3d-8de4-513c22c302e0",
   "metadata": {},
   "source": [
    "# PolicyEngine's Survey Reweighting - The Theory and Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216cb6f2-bdbf-4dba-b9e8-399fe13db9ef",
   "metadata": {},
   "source": [
    "## Totals of Finite Populations\n",
    "\n",
    "### What really is a total?\n",
    "In introductory Statistics, we are introduced to models of data such as $X_i \\sim \\text{N}(\\mu, \\sigma^2)$ or $Y_i \\sim \\text{Bernoulli}(p)$, for an i.i.d. sample $i=1, \\dots, n$. The sample total, $\\sum_{i=1}^n X_i$ or $\\sum_{i=1}^n Y_i$, is noteable in its sufficiency for estimation of $\\mu$ or $p$, but aside from using it to create an unbiased estimator of $\\mu$ or $p$, it's hard to imagine a situation where the sample statistic by itself is especially meaningful. For example, if $Y_i$ is the income of household $i$ and $n$ is 100, then the total is the sum of the incomes of 100 households. It's clear that it's meaningless.\n",
    "\n",
    "On the other hand, imagine that the households are the complete list of households in the United States. Then the total is the aggregate income of all households in the US in a given year, an important quantity to economists. What is fundamentally different about this example?\n",
    "\n",
    "### Finite populations\n",
    "The difference is that in the second example, the population is *finite*. That might not sound like a big deal, or it might even sound like a limitation, but think about why the total lacks intuitive meaning in the first example. By way of these continuous and discrete distributions being *infinite population* models, the individuals drawn are arbitrary. You could draw 100 or 1000 or a trillion other individuals just like them, and in that sense there is no real concept of a total in the infinite population problem formulation.\n",
    "\n",
    "When there is a finite population, there is a clear concept of a total. When we estimate the total of a finite population, the parameter is a function of the potentially observed data, $T = y_1 + y_2 + \\ldots + y_N$, which is also strange when one is used to estimating theoretical parameters than describe the generation of the data. A final unique feature of the finite population approach is that, if all $y_i$ are observed, $T$ is estimated perfectly, with no uncertainty at all. This is the basis for the finite population correction factor when computing the standard error of estimates for totals.\n",
    "\n",
    "### Sampling from finite populations\n",
    "In infinite-population models like the normal and bernoulli models introduced at the beginning, values are drawn from distributions, but there is no concept of a probability of selection of an individual, and, if we insisted upon one, it would be zero. In the finite population sampling case, probabilities of selection are non-zero and determined by the mechanistic design of the sampling process.\n",
    "\n",
    "Perhaps the best known mechanism is randomly sampling without replacement from a population, which in the real world could be approximately implemented with shuffled paper entries in a hat. (Here \"random\" refers to equally likely and not of a random variable.) This is *Simple Random Sampling without Replacement*, often just called *Simple Random Sampling* (SRS). When the population size is $N$ and the sample size is $n$, it can be easily proven that each unit has a probability of selection of $\\pi_i = n / N$.\n",
    "\n",
    "If an SRS is taken separately for two or more groups, also called \"strata,\" this becomes *Stratified Random Sampling*. Assuming different stratum sample sizes, the probabilities of selection are no longer equal. Considering two strata of size $N_1$ and $N_2$ such that population size $N=N_1 + N_2$, and sample sizes of $n_1$ and $n_2$ respectively, the probabilities of selection are $n_1 / N_1$ and $n_2 / N_2$.\n",
    "\n",
    "A general purpose estimator for finite population totals with known probabilities of selection is the Horvitz-Thompson estimator, or $$\\hat{T} = \\sum_{i=1}^n y_i / \\pi_i,$$ where $\\pi_i$ is the probability that unit $i$ is in the sample according to the design. If we define a weight as $w_i = 1 / \\pi_i$, then the Horvitz-Thompson estimator becomes $$\\hat{T} = \\sum_{i=1}^n w_i y_i,$$ and, given probabilities are between 0 and 1, arrives at the interpretation of weights $w_i$ as the number of units in the population each unit in the sample represents, which will be at minimum 1, and, ignoring a zero selection probability for the moment, has no upper bound.\n",
    "\n",
    "### Finite population sampling allows for nonparametric design-based inference\n",
    "\n",
    "Because, unlike the infinite population situation, there is a sampling method that enables units to be selected with known probabilities, a method of inference is available that is virtually assumption-free. This is called \"design-based inference.\" The reason that this is possible is that the random variables that are modeled are not the data, $y_i$, but rather the binary selection indicators $Z_i$ that determine whether unit $i$ was selected by the sampling mechanism. Yes, $Z_i$ is itself a bernouli random variable with $\\Pr(Z_i = 1) = \\pi_i$. Treating $y_i$ as fixed, unknown constants, we can see that the expectation of the Horvitz-Thompson estimator is $$ \\text{E}(\\sum_{i=1}^n y_i / \\pi_i) = \\sum_{i=1}^N \\text{E}(Z_i y_i / \\pi_i),$$ where, notice how the inclusion of the $Z_i$ allows the entire population to enter in the sum. The linearity of the expectation allows it to distribute through the sum, and since $y_i$ and $\\pi_i$ are constants, we have $$\\sum_{i=1}^N \\text{E}(Z_i y_i / \\pi_i) = \\sum_{i=1}^N \\text{E}(Z_i) y_i / \\pi_i = \\sum_{i=1}^N \\pi_i y_i / pi_i$$, where the last inequality follows because $\\text{E}(Z_i) = \\Pr(Z_i = 1)$. Thus, the $\\pi_i$ terms cancel out and we can see that the expectation of the Horvitz-Thompson estimator is the population total $T$ (over all $i = 1, \\ldots, N$).\n",
    "\n",
    "### Pros and cons of the design-based approach\n",
    "\n",
    "#### Pros: Automated analysis that you can trust\n",
    "Just because design-based finite population sampling theory gives us a way out of modeling our metric data, $y_i$, doesn't mean we have to use it. As we'll see, we still can, and perhaps still should, model our data.\n",
    "\n",
    "But if you're building an automated system of analysis and you have access to inclusion probabilities (or equivalently, sample weights), you have the ability to build a hands-free (really head-free) analysis platform. Since the probabilities are at the unit level, the total for any variable measured on the unit, $y_i$, or $x_i$, or $z_i$, whether one is normally distributed and one is binomial and one is cauchy, the exact same formulas and procedures apply. This may sound like what you get with the central limit theorem in infinite-population problem formulations, but this is true at every sample size, and for any method that is derived within the finite-population framework. Furthermore, the cauchy distributed variable was added because it is a pathological case where the central limit theorem breaks down. But the values produced by the draws from the cauchy distribution are no problem for design-based inference, in theory at least, for they are merely constants that make up part of a total.\n",
    "\n",
    "#### Cons: Higher variability esimation, sometimes wildly higher\n",
    "In his 1971 article \"An Essay on the Logical Foundations of Survey Sampling\" (TODO - check), Indian statistician Debabrata Basu introduced a scenario where, despite being unbiased, the Horvitz-Thompson estimator yields absurd results due to high variance.\n",
    "\n",
    "In the example, a circus owner wants to estimate the total weight of 50 elephants. For simplicity, he considers weighing just one elephant—Sambo—known to be roughly average, and multiplying that weight by 50. A statistician, aiming for a more \"rigorous\" design-based method, proposes selecting Sambo with probability 99/100, and distributing the remaining 1/100 equally across the other 49 elephants (each with inclusion probability 1/4900). If Sambo is selected and weighs 4,000 pounds, the Horvitz-Thompson estimate is $4,000 \\cdot 1\\ 0.99 \\approx 4,040$\n",
    "pounds for the total. If instead the largest elephant, Jumbo, is selected and weighs 5,000 pounds, the estimate becomes $ 5,000 \\cdot 1 / (1/4900) = 24,500,000$ pounds. Although these two possible outcomes are wildly different, the expected value of the estimator across all possible samples still equals the true total weight.\n",
    "\n",
    "The method, as rediculous as it is, can still be unbiased because a rare, very large estimate is averaged with frequent very small estimates, and the average across all possible samples is the true average. But that comes as little consolation when there is only one sample selected. The variance of the estimator matters.\n",
    "\n",
    "The design is also intentionally poor in a way that no serious survey would ever be. By overconcentrating inclusion probability on a single unit (Sambo), the design ensures high between-sample variability: in the rare event that Sambo is not selected, the Horvitz-Thompson estimate becomes extreme. This happens not only when Jumbo is selected—causing the estimate to explode—but also when any of the other 48 elephants is selected, due to their tiny inclusion probabilities. As Basu famously remarked, \"A bad design cannot be rescued by a good estimator, but a good estimator can be ruined by a bad design.\"\n",
    "\n",
    "### Models of the data\n",
    "\n",
    "Up until this section, the models of the data, to the extent there were any at all, were models of the sampling process via the random selection indicator $Z_i$. But what if we insisted on modeling the $y_i$s? Our job is still the same, to estimate the population total with only a sample. But, with a model of $y_i$ we feel comfortable with, say $$y_i = \\tau_1 \\text{I}(i \\in \\text{Stratum 1}) + \\tau_2 \\text{I}(i \\in \\text{Stratum 2}) + \\epsilon_i,$$ where $\\epsilon_i \\sim N(0, \\sigma^2)$, we can pursue a different strategy. Our model assumes that, within a stratum, the unit that's selected is indeed arbitrary. Note the use of the infinite-population model, called a \"superpopulation model,\" for $\\epsilon_i$. So if we believe this model, every unit sampled in Stratum 1 and Stratum 2 as good as a random draw from an infinite population. And thus, we can estimate model parameters $\\tau_1$, $\\tau_2$, and $\\sigma^2$ from our sample using ordinary regression techniques.\n",
    "\n",
    "So now you have a fitted model from a sample, how does that get you to estimating a total? Well, you know how many units you didn't sample ($N_1 - n_1$ and $N_2 - n_2$), and you have a model for those units, so you can use the model to predict the missing values. This is essentially missing data imputation. So the estimate of the total would become $$\\text{E}(\\sum_{i=1}^N y_i) = \\sum_{i \\in S} y_i + \\sum_{i \\notin S} \\text{E}(y_i),$$ where $\\E(y_i)$ is the expectation *with respect to the model* (and not the sampling mechanism).\n",
    "\n",
    "In the case of stratified random sampling and this particular model, the model-based and design-based estimators coincide in form. However, the estimated variance differs, and more importantly, the interpretation of the inference differs: the design-based approach conditions on the data and averages over the sampling design, while the model-based approach conditions on the design and assumes the model generates the data. If the model were specified differently—say, using covariates rather than strata—then the model-based estimator would generally differ in form from the design-based estimator.\n",
    "\n",
    "### Weights in model-based inference\n",
    "\n",
    "In model-based analysis, we typically assume that, given the covariates, each observation is equally informative and errors have constant variance. Under these conditions, unweighted estimators are unbiased and, by the Gauss–Markov theorem, have the smallest variance among all linear unbiased estimators. If the model explicitly includes heteroskedastic errors with known or accurately modeled variances, weighting each observation by the inverse of its variance becomes optimal, as this restores efficiency and provides the lowest possible variance for a linear unbiased estimator.\n",
    "\n",
    "In contrast, using weights when each observation is equally informative and errors have constant variance will inflate variance of the estimators considered. This is captured by the \"design-effect\" formula $$1 + \\text{CV}(w)^2,$$ where $\\text{CV}$ is the coeffient of variation. So, absent a clear reason to use unequal weights, when dealing with model-based estimators, it is generally best to use equal weights (i.e., to not use them at all). \n",
    "\n",
    "It's not that weights and models never work together. \"Doubly Robust\" models use design-based weights together with a model of the data to get \"two chances to get it right.\" Population model parameters can be estimated in a design-based way, though the estimate represents the value of the parameter estimate if the same estimation procedure was applied to the whole population. (This is somewhat controversial, since, if a model's parameters are worth estimating in the population, why not use it as a superpopulation model?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a2f6d-1cdd-4dc5-b0c0-7f4fad152603",
   "metadata": {},
   "source": [
    "## Stratified Sampling Example: Design-Based vs. Model-Based Estimation\n",
    "\n",
    "In the code blocks that follow, notice how the design-based and model-based estimators both lead to the same estimate, even when the latter approach did not use weights. The variances are different under the different models, but the current focus is point estimation.\n",
    "\n",
    "\\begin{array}{rcccc}\n",
    "\\textbf{unit} & \\textbf{stratum} & \\textbf{y} & \\textbf{pr\\_select} & \\textbf{w} \\\\\n",
    "\\hline\n",
    "1 & A & 4 & 0.666667 & 1.5 \\\\\n",
    "2 & A & 3 & 0.666667 & 1.5 \\\\\n",
    "3 & A & 5 & 0.666667 & 1.5 \\\\\n",
    "4 & B & 8 & 0.500000 & 2.0 \\\\\n",
    "5 & B & 9 & 0.500000 & 2.0 \\\\\n",
    "6 & B & 7 & 0.500000 & 2.0 \\\\\n",
    "7 & B & 11 & 0.500000 & 2.0 \\\\\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d024f33-8d19-4eb0-bd50-dc0ed7491eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   unit stratum   y  pr_select    w\n",
      "0     1       A   4   0.666667  1.5\n",
      "1     2       A   3   0.666667  1.5\n",
      "2     3       A   5   0.666667  1.5\n",
      "3     4       B   8   0.500000  2.0\n",
      "4     5       B   9   0.500000  2.0\n",
      "5     6       B   7   0.500000  2.0\n",
      "6     7       B  11   0.500000  2.0\n",
      "The Population total of y is 47\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "population_df = pd.DataFrame(\n",
    "    {\n",
    "        \"unit\": [1, 2, 3, 4, 5, 6, 7],\n",
    "        \"stratum\": [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"],\n",
    "        \"y\": [4, 3, 5, 8, 9, 7, 11]\n",
    "    }\n",
    ")\n",
    "\n",
    "N_a = 3\n",
    "N_b = 4\n",
    "\n",
    "n_a = 2\n",
    "n_b = 2\n",
    "\n",
    "# We have to sample to make it true\n",
    "population_df[\"pr_select\"] = np.concatenate([\n",
    "    np.repeat(n_a / N_a, N_a),\n",
    "    np.repeat(n_b / N_b, N_b)\n",
    "])\n",
    "population_df[\"w\"] = 1 / population_df[\"pr_select\"]\n",
    "\n",
    "print(population_df)\n",
    "print(f\"The Population total of y is {np.sum(population_df.y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b608ece1-a360-4dc7-bf06-95b2a3e2bd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   unit stratum   y  pr_select    w\n",
      "0     1       A   4   0.666667  1.5\n",
      "1     2       A   3   0.666667  1.5\n",
      "4     5       B   9   0.500000  2.0\n",
      "6     7       B  11   0.500000  2.0\n",
      "Horvitz-Thompson total estimate: 50.5\n"
     ]
    }
   ],
   "source": [
    "# keeping it simple with n = n_a = n_b = 2\n",
    "sample = (\n",
    "    population_df\n",
    "        .groupby('stratum')\n",
    "        .sample(n=2, random_state=42)\n",
    ")\n",
    "\n",
    "print(sample)\n",
    "\n",
    "ht_total = (sample['y'] * sample['w']).sum()\n",
    "print(\"Horvitz-Thompson total estimate:\", ht_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "415a8dd0-d0c3-4927-b6c4-e7618ac21722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model-based total estimate: 50.5\n"
     ]
    }
   ],
   "source": [
    "# OLS for the stratified model is just the stratum means\n",
    "stratum_means = (\n",
    "    sample.groupby(\"stratum\")[\"y\"]\n",
    "    .mean()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "sample_sum = np.sum(sample.y)\n",
    "N_a_not_sampled = N_a - n_a\n",
    "\n",
    "not_sampled_sum_hat = (\n",
    "    N_a_not_sampled * stratum_means[\"A\"]\n",
    "    + N_b_not_sampled * stratum_means[\"B\"]\n",
    ")\n",
    "model_based_total = sample_sum + not_sampled_sum_hat\n",
    "\n",
    "print(f\"Model-based total estimate: {model_based_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f438a80-15d5-411c-bc2e-04b8f5f34f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
